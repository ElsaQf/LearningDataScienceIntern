{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2, chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/data.csv')\n",
    "data_train, data_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = df.drop('bad', axis=1)\n",
    "num_features = list(data_x.dtypes[~(data_x.dtypes == object)].index)\n",
    "cat_features = list(data_x.dtypes[data_x.dtypes == object].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_maxvalue_ = 5\n",
    "max_interval_ = 5  # 分箱后的箱数\n",
    "IV_threshold_ = 0.1\n",
    "cor_threshold_ = 0.6\n",
    "VIF_threshold_ = 5\n",
    "ANOVA_threshold_ = 0.01\n",
    "RFE_ratio_ = 0.85\n",
    "select_model_rario_ = 0.5\n",
    "pvalues_threshold_ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查类别变量中哪些变量取值超过 5（通过cat_maxvalue常量来设置）\n",
    "more_value_features = []\n",
    "less_value_features = []\n",
    "for var in ['f1','f2','f3']:\n",
    "    value_count = len(set(data_train[var]))\n",
    "    if value_count > cat_maxvalue_:\n",
    "        more_value_features.append(var)\n",
    "    else:\n",
    "        less_value_features.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "卡方分箱\n",
    "WOE\n",
    "IV\n",
    "\"\"\"\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "# from scipy.stats import chi2, chi2_contingency\n",
    "\n",
    "def bin_bad_rate(df, col, target):\n",
    "    \"\"\"\n",
    "    计算坏样本率\n",
    "    :param df: 需要计算好坏比率的数据集\n",
    "    :param col: 需要计算好坏比率的特征\n",
    "    :param target: 好坏标签\n",
    "    :return: 每箱的坏样本率\n",
    "    \"\"\"\n",
    "    total = df.groupby(by=col, sort=True)[target].count()\n",
    "    total = pd.DataFrame({'total': total})\n",
    "    bad = df.groupby(by=col, sort=True)[target].sum()\n",
    "    bad = pd.DataFrame({'bad': bad})\n",
    "    regroup = total.merge(bad, left_index=True, right_index=True, how='left')\n",
    "    regroup['good'] = regroup['total'] - regroup['bad']\n",
    "    regroup.reset_index(level=0, inplace=True)\n",
    "    regroup['bad_rate'] = regroup['bad'] / regroup['total']\n",
    "    return regroup\n",
    "\n",
    "\n",
    "def merge_cat_bin(df, col, target, direction='bad'):\n",
    "    \"\"\"\n",
    "    类别变量合并\n",
    "    合并逻辑：坏样本率为0时，按照从小到大排序依次合并到下一个类别中直到合并后的类别坏样本率大于0\n",
    "            坏样本率为1时，按照从大到小排序依次合并到下一个类别中直到合并后的类别坏样本率小于1\n",
    "    :param df: 包含检验0%或100%的坏样本率的数据集\n",
    "    :param col: 需要合并的类别变量\n",
    "    :param target: 目标变量，1表示坏\n",
    "    :param direction: 用来区别坏样本率为0或1\n",
    "    :return: 合并结果，使得每个组里同时包含好坏样本\n",
    "    \"\"\"\n",
    "    regroup = bin_bad_rate(df, col, target)\n",
    "    if direction == 'bad':\n",
    "        regroup = regroup.sort_values(by='bad_rate')\n",
    "    else:\n",
    "        regroup = regroup.sort_values(by='bad_rate', ascending=False)\n",
    "    regroup_col = [[i] for i in regroup[col]]\n",
    "    del_index = []\n",
    "    for i in range(regroup.shape[0]-1):\n",
    "        regroup_col[i+1] = regroup_col[i] + regroup_col[i+1]\n",
    "        del_index.append(i)\n",
    "        if direction == 'bad':\n",
    "            if regroup['bad_rate'][i+1] > 0:\n",
    "                break\n",
    "        else:\n",
    "            if regroup['bad_rate'] < 1:\n",
    "                break\n",
    "    regroup_col2 = [regroup_col[i] for i in range(len(regroup_col)) if i not in del_index]\n",
    "    new_group = {}\n",
    "    for i in range(len(regroup_col2)):\n",
    "        for g2 in regroup_col2[i]:\n",
    "            new_group[g2] = 'bin' + str(i)\n",
    "    return new_group\n",
    "\n",
    "\n",
    "def split_data(df, col, num_split):\n",
    "    \"\"\"\n",
    "    切分逻辑：等频分成100份，再将切割点去重\n",
    "    :param df: 包含需要待分箱特征的数据集\n",
    "    :param col: 待分箱的变量\n",
    "    :param num_split: 切分的组别数\n",
    "    :return: col中切分点（在原数据集上增加一列，把原始细粒度的col重新划分成粗粒度的值，便于后面分箱的合并处理）\n",
    "    \"\"\"\n",
    "    N = df.shape[0]\n",
    "    n = int(N / num_split)\n",
    "    split_point_index = [i * n - 1 for i in range(1, num_split)]\n",
    "    raw_values = sorted(df[col])\n",
    "    split_point = [raw_values[i] for i in split_point_index]\n",
    "    split_point = sorted(set(split_point))\n",
    "    return split_point\n",
    "\n",
    "\n",
    "def assign_group(x, bin, value_max):\n",
    "    \"\"\"\n",
    "    分配逻辑：对于在区间(a, b]内的数值，取右侧的值作为这个区间所有值的表征；\n",
    "            对于大于切分点最大值的，可随意取一个较大的数作为表征\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param bin: 上述变量的分箱结果\n",
    "    :param value_max: 这列数据的最大值\n",
    "    :return: x在分箱结果下的映射\n",
    "    \"\"\"\n",
    "    N = len(bin)\n",
    "    if x <= min(bin):\n",
    "        return min(bin)\n",
    "    elif x > max(bin):\n",
    "        return value_max + 1\n",
    "    else:\n",
    "        for i in range(N-1):\n",
    "            if bin[i] < x <= bin[i+1]:\n",
    "                return bin[i+1]\n",
    "\n",
    "\n",
    "def chisq(df):\n",
    "    \"\"\"\n",
    "    计算卡方值\n",
    "    :param df: 包含各类别全部样本总计和各类别坏样本总计的数据框\n",
    "    :return: 卡方值\n",
    "    \"\"\"\n",
    "    # 经过Yate修正\n",
    "    bad_rate = sum(df['bad']) / sum(df['total'])\n",
    "    if bad_rate in [0, 1]:\n",
    "        return 0\n",
    "    df['good'] = df['total'] - df['bad']\n",
    "    good_rate = sum(df['good']) / sum(df['total'])\n",
    "    df['bad_expected'] = df['total'] * bad_rate\n",
    "    df['good_expected'] = df['total'] * good_rate\n",
    "    bad_combined = zip(df['bad_expected'], df['bad'])\n",
    "    good_combined = zip(df['good_expected'], df['good'])\n",
    "    bad_chi = [(abs(i[0] - i[1]) - 0.5)**2 / i[0] for i in bad_combined]\n",
    "    good_chi = [(abs(i[0] - i[1]) - 0.5)**2 / i[0] for i in good_combined]\n",
    "    chisq_s = sum(bad_chi) + sum(good_chi)\n",
    "    return chisq_s\n",
    "\n",
    "\n",
    "def chisq_v2(df):\n",
    "    # 计算卡方值_v2\n",
    "    bad_rate = sum(df['bad']) / sum(df['total'])\n",
    "    if bad_rate in [0, 1]:\n",
    "        return 0\n",
    "    else:\n",
    "        chi = chi2_contingency(df[['bad', 'good']])[0]\n",
    "        return chi\n",
    "\n",
    "\n",
    "def assign_bin(x, cut_points, special_attribute=None):\n",
    "    \"\"\"\n",
    "    根据切分点分箱\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param cut_points: 上述变量的分箱结果，用切分点表示\n",
    "    :param special_attribute: 不参与分箱的特殊取值\n",
    "    :return: 分箱后对应的第几个箱子\n",
    "    \"\"\"\n",
    "    if special_attribute is None:\n",
    "        special_attribute = []\n",
    "    num_bin = len(cut_points)\n",
    "    if x in special_attribute:\n",
    "        i = special_attribute.index(x) + 1\n",
    "        return 'Bin_{}'.format(0 - i)\n",
    "    if x <= cut_points[0]:\n",
    "        return 'Bin_0'\n",
    "    elif x > cut_points[-1]:\n",
    "        return 'Bin_{}'.format(num_bin)\n",
    "    else:\n",
    "        for i in range(0, num_bin - 1):\n",
    "            if cut_points[i] < x <= cut_points[i + 1]:\n",
    "                return 'Bin_{}'.format(i + 1)\n",
    "\n",
    "\n",
    "def chi_merge(df, col, target, max_interval=5, special_attribute=None, set_threshold=True):\n",
    "    \"\"\"\n",
    "    卡方分箱\n",
    "    :param df: 包含目标变量与分箱属性的数据框\n",
    "    :param col: 需要分箱的属性\n",
    "    :param target: 目标变量\n",
    "    :param max_interval: 最大分箱数，如果原始属性的取值个数低于该参数，不执行这段函数\n",
    "    :param special_attribute: 不参与分箱的取值\n",
    "    :param set_threshold: 是否设置合并的卡方上限\n",
    "    :return: 分箱结果\n",
    "    \"\"\"\n",
    "    if special_attribute is None:\n",
    "        special_attribute = []\n",
    "    if len(special_attribute) > 0:\n",
    "        df2 = df[~(df[col].isin(special_attribute))]  # 去掉special_attribute后的df\n",
    "    else:\n",
    "        df2 = df.copy()\n",
    "    col_values = sorted(set(df2[col]))\n",
    "    n_distinct = len(col_values)\n",
    "    if n_distinct <= max_interval - len(special_attribute):  # 如果原始属性的取值个数低于max_interval，不执行卡方分箱的逻辑\n",
    "        cut_points = col_values[:-1]\n",
    "        df2['temp'] = df2[col]\n",
    "    else:\n",
    "        # 1. 将col按等频分成100份，如果col中去重之后的个数小于100直接进行下面的步骤\n",
    "        if n_distinct > 100:\n",
    "            split_x = split_data(df2, col, 100)\n",
    "            col_max = max(df2[col])\n",
    "            # assign_group函数：返回原值在切分后的映射，经map后，生成该特征初步分箱后的结果\n",
    "            df2['temp'] = df2[col].map(lambda x: assign_group(x, split_x, col_max))\n",
    "        else:\n",
    "            df2['temp'] = df2[col]\n",
    "        # 计算初步分箱后的bad rate，总体bad rate将被用来计算expected bad count\n",
    "        regroup = bin_bad_rate(df2, 'temp', target)\n",
    "        # 首先，每个单独的属性值将被分为单独的一组，然后两两组别进行合并\n",
    "        group_intervals = [[i] for i in regroup['temp']]\n",
    "\n",
    "        # 2. 建立循环，不断合并最优的相邻两个组别，直到：\n",
    "        # （1）最优的两个组别的卡方值大于合并阈值，或最终分裂出来的分箱数<=预设的最大分箱数\n",
    "        # （2）每箱同时包含好坏样本\n",
    "        # （3）每箱的占比不低于预设值（可选，这里没设置）\n",
    "        # 如果有特殊属性，那么最终分裂出来的分箱数 = 设的最大分箱数 - 特殊属性的个数\n",
    "        split_intervals = max_interval - len(special_attribute)\n",
    "        # 合并的卡方阈值\n",
    "        if set_threshold:\n",
    "            threshold = chi2.isf(0.05, 1)\n",
    "        else:\n",
    "            threshold = 999\n",
    "#         time0 = time.time()\n",
    "        while len(group_intervals) > split_intervals:\n",
    "#             time1 = time.time()\n",
    "            # 每次循环计算合并相邻组别后的卡方值\n",
    "            chi_list = []\n",
    "            for k in range(len(group_intervals)-1):\n",
    "                temp_group = [min(group_intervals[k]), min(group_intervals[k+1])]\n",
    "                df2g = regroup.loc[regroup['temp'].isin(temp_group)][['total', 'bad', 'good']]\n",
    "                # chi = chisq(df2g)\n",
    "                chi = chisq_v2(df2g)\n",
    "                chi_list.append(chi)\n",
    "            # time2 = time.time()\n",
    "            # print(time2 - time1)\n",
    "            chi_min = min(chi_list)\n",
    "            if chi_min <= threshold:\n",
    "                best_combined = chi_list.index(chi_min)\n",
    "                # 更新group_intervals\n",
    "                group_intervals[best_combined] = group_intervals[best_combined] + group_intervals[best_combined+1]\n",
    "                group_intervals.remove(group_intervals[best_combined+1])\n",
    "                # 更新regroup\n",
    "                regroup.iloc[best_combined, 1:] = regroup.iloc[best_combined, 1:] + regroup.iloc[best_combined+1, 1:]\n",
    "                drop_index = regroup.index[best_combined + 1]\n",
    "                regroup.drop(drop_index, inplace=True)\n",
    "                # time3 = time.time()\n",
    "                # print(time3 - time2)\n",
    "            else:\n",
    "                break\n",
    "        # time4 = time.time()\n",
    "        # print(time4 - time0)\n",
    "        cut_points = [max(i) for i in group_intervals[:-1]]\n",
    "    # 检查是否有箱没有好或者坏样本。如果有，需要跟相邻的箱进行合并，直到每箱同时包含好坏样本\n",
    "    df2['temp_bin'] = df2['temp'].apply(lambda x: assign_bin(x, cut_points))\n",
    "    regroup = bin_bad_rate(df2, 'temp_bin', target)\n",
    "    bbr_values = regroup['bad_rate']\n",
    "    min_bad_rate = min(bbr_values)\n",
    "    max_bad_rate = max(bbr_values)\n",
    "    while min_bad_rate == 0 or max_bad_rate == 1:\n",
    "        # 找出全部为好/坏样本的箱子\n",
    "        bin_bad_01 = regroup[regroup['bad_rate'].isin([0, 1])].temp_bin.tolist()\n",
    "        bin0 = bin_bad_01[0]\n",
    "        bin_max = max([i.split('_')[1] for i in regroup.temp_bin])\n",
    "        bin_min = min([i.split('_')[1] for i in regroup.temp_bin])\n",
    "        # 如果是最后一箱, 则需要和上一个箱进行合并，分裂点cut_points中的最后一个需要移除\n",
    "        if bin0.split('_')[1] == bin_max:\n",
    "            del cut_points[-1]\n",
    "        # 如果是第一箱，则需要和下一个箱进行合并, 分裂点cut_points中的第一个需要移除\n",
    "        elif bin0.split('_')[1] == bin_min:\n",
    "            del cut_points[0]\n",
    "        # 如果是中间的某一箱，则需要和前后中的一个箱进行合并，依据是较小的卡方值\n",
    "        else:\n",
    "            # 和前一箱进行合并，并计算卡方值\n",
    "            current_index = list(regroup.temp_bin).index(bin0)\n",
    "            prev_bin = list(regroup.temp_bin)[current_index - 1]\n",
    "            df2g = regroup.loc[regroup['temp_bin'].isin([prev_bin, bin0])][['total', 'bad', 'good']]\n",
    "            chisq1 = chisq_v2(df2g)\n",
    "            # 和后一箱进行合并，并计算卡方值\n",
    "            later_bin = list(regroup.temp_bin)[current_index + 1]\n",
    "            df2g = regroup.loc[regroup['temp_bin'].isin([later_bin, bin0])][['total', 'bad', 'good']]\n",
    "            chisq2 = chisq_v2(df2g)\n",
    "            if chisq1 < chisq2:\n",
    "                cut_points.remove(cut_points[current_index - 1])\n",
    "            else:\n",
    "                cut_points.remove(cut_points[current_index])\n",
    "        # 完成一次合并后，需要再次计算新的分箱准则下，每箱是否同时包含好坏样本\n",
    "        if len(cut_points) > 0:\n",
    "            df2['temp_bin'] = df2['temp'].apply(lambda x: assign_bin(x, cut_points))\n",
    "            regroup = bin_bad_rate(df2, 'temp_bin', target)\n",
    "            bbr_values = regroup['bad_rate']\n",
    "            min_bad_rate = min(bbr_values)\n",
    "            max_bad_rate = max(bbr_values)\n",
    "        else:\n",
    "            return special_attribute + [1e6]\n",
    "    cut_points = special_attribute + cut_points\n",
    "    return cut_points\n",
    "\n",
    "\n",
    "def calc_woe_iv(df, col, target):\n",
    "    \"\"\"\n",
    "    计算分箱后的WOE和IV\n",
    "    :param df: 包含需要计算WOE的变量和目标变量\n",
    "    :param col: 需要计算WOE、IV的变量，分箱后的变量，或不需要分箱的类别变量\n",
    "    :param target: 目标变量，1为坏样本\n",
    "    :return: 返回WOE和IV\n",
    "    \"\"\"\n",
    "    regroup = bin_bad_rate(df, col, target)\n",
    "    N = sum(regroup['total'])\n",
    "    B = sum(regroup['bad'])\n",
    "    G = N - B\n",
    "    regroup['bad_percent'] = regroup['bad'].map(lambda x: x / B)\n",
    "    regroup['good_percent'] = regroup['good'].map(lambda x: x / G)\n",
    "    regroup['WOE'] = regroup.apply(lambda x: np.log(x.bad_percent/x.good_percent), axis=1)\n",
    "    woe_dict = dict(zip(regroup[col], regroup['WOE']))\n",
    "    regroup['IV'] = regroup.apply(lambda x: (x.bad_percent - x.good_percent) * x.WOE, axis=1)\n",
    "    IV = regroup['IV'].sum()\n",
    "    return woe_dict, IV\n",
    "\n",
    "\n",
    "def data_test_process(df, cm_dict, bre_dict, com_dict, WOE_dict):\n",
    "    \"\"\"\n",
    "    处理测试集，返回woe编码后的测试集\n",
    "    :param df: test data\n",
    "    :param cm_dict: <= 5 的类别特征中存在某个类别只包含好样本或坏样本，合并之后的变量\n",
    "    :param bre_dict: 大于5的类别特征，bad rate编码\n",
    "    :param com_dict: 连续变量分箱结果，其中包括bad rate编码之后的类别特征\n",
    "    :param WOE_dict: woe编码结果\n",
    "    :return: 经过woe编码之后的test data\n",
    "    \"\"\"\n",
    "    if len(cm_dict) > 0:\n",
    "        for col in cm_dict.keys():\n",
    "            bin_var = col + '_bin'\n",
    "            df[bin_var] = df[col].map(cm_dict[col])\n",
    "    if len(bre_dict) > 0:\n",
    "            for col in bre_dict.keys():\n",
    "                br_var = col + '_br_encoding'\n",
    "                df[br_var] = df[col].map(bre_dict[col])\n",
    "    if len(com_dict) > 0:\n",
    "            for col in com_dict.keys():\n",
    "                bin_var = col + '_bin'\n",
    "                if -99999 not in set(df[col]):\n",
    "                    df[bin_var] = df[col].map(lambda x: assign_bin(x, com_dict[col]))\n",
    "                else:\n",
    "                    df[bin_var] = df[col].map(lambda x: assign_bin(x, com_dict[col], special_attribute=[-99999]))\n",
    "    if len(WOE_dict) > 0:\n",
    "            for col in WOE_dict.keys():\n",
    "                WOE_var = col + '_WOE'\n",
    "                df[WOE_var] = df[col].map(WOE_dict[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "def roc_curve(df, target, prob, thresholds):\n",
    "    \"\"\"\n",
    "    计算fpr, tpr\n",
    "    :param df: 包含目标变量和预测结果的数据集\n",
    "    :param target: 目标变量\n",
    "    :param prob: 预测为正样本的概率\n",
    "    :param thresholds: 由训练数据得出的阈值\n",
    "    :return: fpr, tpr\n",
    "    \"\"\"\n",
    "    data = df[[target, prob]]\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    for t in thresholds:\n",
    "        data['predict_cat'] = data[prob].apply(lambda x: 1 if x > t else 0)\n",
    "        data['true'] = (data[target] == data['predict_cat']) * 1\n",
    "        TP = len(data[(data['true'] == 1) & (data['predict_cat'] == 1)])\n",
    "        TN = len(data[(data['true'] == 1) & (data['predict_cat'] == 0)])\n",
    "        FP = len(data[(data['true'] == 0) & (data['predict_cat'] == 1)])\n",
    "        FN = len(data[(data['true'] == 0) & (data['predict_cat'] == 0)])\n",
    "        fpr.append(FP / (FP + TN))\n",
    "        tpr.append(TP / (TP + FN))\n",
    "    return fpr, tpr\n",
    "\n",
    "\n",
    "def bad_rate_monotone(df, sort_var, target, special_attribute=None):\n",
    "    \"\"\"\n",
    "    坏样本率是否单调\n",
    "    检验逻辑：两个箱子一定单调；两个以上时，检验是否有峰值点（大于相邻点的值）和谷值点（小于相邻点的值）\n",
    "    :param df: 包含检验坏样本率的变量，和目标变量\n",
    "    :param sort_var: 需要检验坏样本率的变量\n",
    "    :param target: 目标变量，0、1表示好、坏\n",
    "    :param special_attribute: 不参与检验的特殊值\n",
    "    :return: 坏样本率单调与否\n",
    "    \"\"\"\n",
    "    if special_attribute is None:\n",
    "        special_attribute = []\n",
    "    if len(special_attribute) > 0:\n",
    "        df2 = df.loc[~df[sort_var].isin(special_attribute)]\n",
    "    else:\n",
    "        df2 = df.copy()\n",
    "    if len(set(df2[sort_var])) <= 2:\n",
    "        return True\n",
    "    regroup = bin_bad_rate(df2, sort_var, target)\n",
    "    bad_rate = list(regroup['bad_rate'])\n",
    "    bad_rate_notmonotone = [bad_rate[i] > bad_rate[i-1] and bad_rate[i] > bad_rate[i+1] or bad_rate[i] < bad_rate[i-1]\n",
    "                            and bad_rate[i] < bad_rate[i+1] for i in range(1, len(bad_rate) - 1)]\n",
    "    if True in bad_rate_notmonotone:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdmall_user_p0066 is in processing\n",
      "jdmall_user_p0001 is in processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdmall_user_p0006 is in processing\n",
      "jdmall_user_p0068 is in processing\n",
      "jdmall_up_m0001 is in processing\n",
      "jdmall_up_m0009 is in processing\n",
      "pay_pay_p0004176 is in processing\n",
      "catenum is in processing\n",
      "catenum_3m is in processing\n",
      "f2_br_encoding is in processing\n",
      "f3_br_encoding is in processing\n",
      "f2_br_encoding is in processing\n",
      "f3_br_encoding is in processing\n",
      "f2_br_encoding is in processing\n",
      "f3_br_encoding is in processing\n"
     ]
    }
   ],
   "source": [
    "# （i）类别变量取值小于5时，如果每种类别同时包含好坏样本，无需分箱；如果某个类别只包含好样本或坏样本，需要合并\n",
    "cat_merged_dict = {}\n",
    "var_bin_list = []\n",
    "del_less_value_features = []\n",
    "for col in less_value_features:\n",
    "    regroup = bin_bad_rate(data_train, col, 'bad')\n",
    "    bbr_values = regroup['bad_rate']  # 每个类别的坏样本占比\n",
    "    if min(bbr_values) == 0:  # 某个类别只包含好样本\n",
    "        combine_bin_dict = merge_cat_bin(data_train, col, 'bad')\n",
    "        cat_merged_dict[col] = combine_bin_dict\n",
    "        new_var = col + '_bin'\n",
    "        data_train[new_var] = data_train[col].map(combine_bin_dict)\n",
    "        var_bin_list.append(new_var)\n",
    "        del_less_value_features.append(col)\n",
    "    if max(bbr_values) == 1:  # 某个类别只包含坏样本\n",
    "        combine_bin_dict = merge_cat_bin(data_train, col, 'bad', direction='good')\n",
    "        cat_merged_dict[col] = combine_bin_dict\n",
    "        new_var = col + '_bin'\n",
    "        data_train[new_var] = data_train[col].map(combine_bin_dict)\n",
    "        var_bin_list.append(new_var)\n",
    "        del_less_value_features.append(col)\n",
    "less_value_features = [col for col in less_value_features if col not in del_less_value_features]\n",
    "\n",
    "# （ii）类别变量取值大于5时，需要bad rate编码，再用卡方分箱法分箱\n",
    "br_encoding_dict = {}  # 记录进行bad rate 编码的变量，及编码方式\n",
    "for col in more_value_features:\n",
    "    regroup = bin_bad_rate(data_train, col, 'bad')\n",
    "    br_encoding = dict(zip(regroup[col], regroup['bad_rate']))\n",
    "    data_train[col + '_br_encoding'] = data_train[col].map(br_encoding)\n",
    "    br_encoding_dict[col] = br_encoding\n",
    "    num_features.append(col + '_br_encoding')\n",
    "\n",
    "# （iii）对连续变量进行分箱，包括（ii）中的变量\n",
    "continuous_merged_dict = {}\n",
    "for col in num_features:\n",
    "    print('{} is in processing'.format(col))\n",
    "    max_interval = max_interval_\n",
    "    if -99999 not in set(data_train[col]):   # 检查是否有特殊值存在，如果没有所有取值都参与分箱\n",
    "        cut_off = chi_merge(data_train, col, 'bad', max_interval=max_interval,\n",
    "                                                set_threshold=False)\n",
    "        new_var = col + '_bin'\n",
    "        data_train[new_var] = data_train[col].map(lambda x: assign_bin(x, cut_off))\n",
    "        monotone = bad_rate_monotone(data_train, new_var, 'bad')\n",
    "        while not monotone:\n",
    "            max_interval -= 1\n",
    "            cut_off = chi_merge(data_train, col, 'bad', max_interval=max_interval,\n",
    "                                                    set_threshold=False)\n",
    "            data_train[new_var] = data_train[col].map(lambda x: assign_bin(x, cut_off))\n",
    "            if max_interval == 2:\n",
    "                # 当分箱数为2时，必然单调\n",
    "                break\n",
    "            monotone = bad_rate_monotone(data_train, new_var, 'bad')\n",
    "        var_bin_list.append(new_var)\n",
    "    else:\n",
    "        cut_off = chi_merge(data_train, col, 'bad', max_interval=max_interval,\n",
    "                                                special_attribute=[-99999], set_threshold=False)\n",
    "        new_var = col + '_bin'\n",
    "        data_train[new_var] = data_train[col].map(lambda x: assign_bin(x, cut_off,\n",
    "                                                                                           special_attribute=[-99999]))\n",
    "        monotone = bad_rate_monotone(data_train, new_var, 'bad', ['bin_-1'])\n",
    "        while not monotone:\n",
    "            max_interval -= 1\n",
    "            cut_off = chi_merge(data_train, col, 'bad', max_interval=max_interval,\n",
    "                                                    special_attribute=[-99999], set_threshold=False)\n",
    "            data_train[new_var] = data_train[col].map(lambda x: assign_bin(x, cut_off,\n",
    "                                                                                               special_attribute=[-99999]))\n",
    "            if max_interval == 3:\n",
    "                # 当分箱数为3-1=2时，必然单调\n",
    "                break\n",
    "            monotone = bad_rate_monotone(data_train, new_var, 'bad', ['bin_-1'])\n",
    "        var_bin_list.append(new_var)\n",
    "    continuous_merged_dict[col] = cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "WOE_dict = {}\n",
    "IV_dict = {}\n",
    "# 分箱后进行WOE和IV编码的变量包括：\n",
    "# 1. 初始取值个数小于5，且不需要合并的类别型变量。存放在less_value_features中\n",
    "# 2. 初始取值个数小于5，需要合并的类别型变量。合并后新的变量存放在var_bin_list中\n",
    "# 3. 初始取值个数超过5，需要合并的类别型变量。合并后新的变量存放在var_bin_list中\n",
    "# 4. 连续变量。分箱后新的变量存放在var_bin_list中\n",
    "all_var = var_bin_list + less_value_features\n",
    "for var in all_var:\n",
    "    WOE_dict[var], IV_dict[var] = calc_woe_iv(data_train, var, 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jdmall_user_p0066_bin': {'Bin_0': -0.10661761131009002,\n",
       "  'Bin_1': 0.06222736524059458},\n",
       " 'jdmall_user_p0001_bin': {'Bin_0': 0.22610984202089826,\n",
       "  'Bin_1': -0.011231373346589517,\n",
       "  'Bin_2': -0.7951157830317914},\n",
       " 'jdmall_user_p0006_bin': {'Bin_0': -0.013267832055195085,\n",
       "  'Bin_1': 1.1858856858347921},\n",
       " 'jdmall_user_p0068_bin': {'Bin_0': -0.03743008133427491,\n",
       "  'Bin_1': 0.4559245321521305},\n",
       " 'jdmall_up_m0001_bin': {'Bin_0': -0.05946068500552629,\n",
       "  'Bin_1': 0.02334382763627756},\n",
       " 'jdmall_up_m0009_bin': {'Bin_0': -0.05998545662265461,\n",
       "  'Bin_1': 0.06137705286310283},\n",
       " 'pay_pay_p0004176_bin': {'Bin_0': -0.032180881567003494,\n",
       "  'Bin_1': 0.09313851661908053},\n",
       " 'catenum_bin': {'Bin_0': 0.02182948891429853, 'Bin_1': -0.6386636062162537},\n",
       " 'catenum_3m_bin': {'Bin_0': 0.018280525679731048,\n",
       "  'Bin_1': -1.0654061127717032},\n",
       " 'f2_br_encoding_bin': {'Bin_0': -0.5633141689744668,\n",
       "  'Bin_1': -0.05730783164442516,\n",
       "  'Bin_2': -0.055098583467963265,\n",
       "  'Bin_3': 0.09249534114783399,\n",
       "  'Bin_4': 0.4777006279103062},\n",
       " 'f3_br_encoding_bin': {'Bin_0': -0.6150904384981871,\n",
       "  'Bin_1': -0.10934870976946863,\n",
       "  'Bin_2': 0.09774469703397791,\n",
       "  'Bin_3': 0.4532076665182113,\n",
       "  'Bin_4': 0.6894487995209011},\n",
       " 'f1': {'a1': -0.1087482894572247,\n",
       "  'a2': 0.4246007001076287,\n",
       "  'a3': 0.13991713065210445}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WOE_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jdmall_user_p0066_bin': 0.006630867506591915,\n",
       " 'jdmall_user_p0001_bin': 0.037143737597943795,\n",
       " 'jdmall_user_p0006_bin': 0.01571399041985139,\n",
       " 'jdmall_user_p0068_bin': 0.017041135241000437,\n",
       " 'jdmall_up_m0001_bin': 0.0013878794530931447,\n",
       " 'jdmall_up_m0009_bin': 0.0036806013629319314,\n",
       " 'pay_pay_p0004176_bin': 0.0029965312028889385,\n",
       " 'catenum_bin': 0.01392562653672294,\n",
       " 'catenum_3m_bin': 0.019445195513510594,\n",
       " 'f2_br_encoding_bin': 0.05161053723936894,\n",
       " 'f3_br_encoding_bin': 0.08909469024434707,\n",
       " 'f1': 0.03792708164362232}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
